{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb2b215-8292-4253-aeaa-43e0ebac9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import random\n",
    "import typing as tp\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# import torchaudio as ta\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "from fairseq import utils\n",
    "from multihead_attention import MultiheadAttention\n",
    "from torch import Tensor\n",
    "from typing import Dict, List, Optional, Any\n",
    "import contextlib\n",
    "from fairseq.models import (\n",
    "    FairseqEncoder,\n",
    "    FairseqIncrementalDecoder\n",
    ")\n",
    "from fairseq.modules.conformer_layer import ConformerEncoderLayer\n",
    "from fairseq.modules import (\n",
    "    FairseqDropout,\n",
    "    LayerNorm,\n",
    "    TransformerEncoderLayer,\n",
    "    PositionalEmbedding,\n",
    "    LayerDropModuleList\n",
    ")\n",
    "from fairseq.modules.checkpoint_activations import checkpoint_wrapper\n",
    "\n",
    "\n",
    "with open('/mnt/petrelfs/zhangchi1/m2t/layout.pkl', 'rb') as f:\n",
    "    loaded_layout = pickle.load(f)\n",
    "\n",
    "class ConvSequence(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: tp.Sequence[int], kernel: int = 3, dilation_growth: int = 2,\n",
    "                 dilation_period: tp.Optional[int] = 5, stride: int = 1,\n",
    "                 dropout: float = 0.0, leakiness: float = 0.0, groups: int = 1,\n",
    "                 decode: bool = False, batch_norm: bool = True, dropout_input: float = 0.0,\n",
    "                 skip: bool = True, scale: tp.Optional[float] = None, rewrite: bool = False,\n",
    "                 activation_on_last: bool = True, post_skip: bool = False, glu: int = 2,\n",
    "                 glu_context: int = 1, glu_glu: bool = True, activation: tp.Any = nn.GELU) -> None:\n",
    "        super().__init__()\n",
    "        dilation = 1\n",
    "        channels = tuple(channels)\n",
    "        self.skip = skip\n",
    "        self.sequence = nn.ModuleList()\n",
    "        self.glus = nn.ModuleList()\n",
    "        if activation is None:\n",
    "            activation = partial(nn.LeakyReLU, leakiness)\n",
    "        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n",
    "        # build layers\n",
    "        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n",
    "            layers: tp.List[nn.Module] = []\n",
    "            is_last = k == len(channels) - 2\n",
    "\n",
    "            # Set dropout for the input of the conv sequence if defined\n",
    "            if k == 0 and dropout_input:\n",
    "                assert 0 < dropout_input < 1\n",
    "                layers.append(nn.Dropout(dropout_input))\n",
    "\n",
    "            # conv layer\n",
    "            if dilation_growth > 1:\n",
    "                assert kernel % 2 != 0, \"Supports only odd kernel with dilation for now\"\n",
    "            if dilation_period and (k % dilation_period) == 0:\n",
    "                dilation = 1\n",
    "            pad = kernel // 2 * dilation\n",
    "            layers.append(Conv(chin, chout, kernel, stride, pad,\n",
    "                               dilation=dilation, groups=groups if k > 0 else 1))\n",
    "            dilation *= dilation_growth # dilation_growth = 2\n",
    "            # non-linearity\n",
    "            if activation_on_last or not is_last:\n",
    "                if batch_norm:\n",
    "                    layers.append(nn.BatchNorm1d(num_features=chout))\n",
    "                layers.append(activation())\n",
    "                if dropout:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                if rewrite:\n",
    "                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n",
    "                    # layers += [nn.Conv1d(chout, 2 * chout, 1), nn.GLU(dim=1)]\n",
    "            if chin == chout and skip:\n",
    "                if scale is not None:\n",
    "                    layers.append(LayerScale(chout, scale))\n",
    "                if post_skip:\n",
    "                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n",
    "\n",
    "            self.sequence.append(nn.Sequential(*layers))\n",
    "            if glu and (k + 1) % glu == 0:\n",
    "                ch = 2 * chout if glu_glu else chout\n",
    "                act = nn.GLU(dim=1) if glu_glu else activation()\n",
    "                self.glus.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n",
    "            else:\n",
    "                self.glus.append(None)\n",
    "\n",
    "    def forward(self, x: tp.Any) -> tp.Any:\n",
    "        for module_idx, module in enumerate(self.sequence):\n",
    "            old_x = x\n",
    "            x = module(x)\n",
    "            if self.skip and x.shape == old_x.shape:\n",
    "                x = x + old_x\n",
    "            glu = self.glus[module_idx]\n",
    "            if glu is not None:\n",
    "                x = glu(x)\n",
    "        return x\n",
    "\n",
    "class PositionGetter:\n",
    "    INVALID = -0.1\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._cache: tp.Dict[int, torch.Tensor] = {}\n",
    "        self._invalid_names: tp.Set[str] = set()\n",
    "\n",
    "    def get_recording_layout(self, layout) -> torch.Tensor:\n",
    "        indexes: tp.List[int] = []\n",
    "        valid_indexes: tp.List[int] = []\n",
    "        for meg_index, name in enumerate(layout.names):\n",
    "            name = name.rsplit(\"-\", 1)[0]\n",
    "            try:\n",
    "                indexes.append(layout.names.index(name))\n",
    "            except ValueError:\n",
    "                if name not in self._invalid_names:\n",
    "                    print(\n",
    "                        \"Channels %s not in layout for recording\",\n",
    "                        name,)\n",
    "                    self._invalid_names.add(name)\n",
    "            else:\n",
    "                valid_indexes.append(meg_index)\n",
    "\n",
    "        positions = torch.full((len(layout.names), 2), self.INVALID)\n",
    "        x, y = layout.pos[indexes, :2].T\n",
    "        x = (x - x.min()) / (x.max() - x.min())\n",
    "        y = (y - y.min()) / (y.max() - y.min())\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        positions[valid_indexes, 0] = x\n",
    "        positions[valid_indexes, 1] = y\n",
    "        return positions\n",
    "\n",
    "    def get_positions(self, meg, batch, layout):\n",
    "        B, C, T = meg.shape\n",
    "        positions = torch.full((B, C, 2), self.INVALID, device=meg.device)\n",
    "        for idx in range(len(batch['subject_index'])):\n",
    "            rec_pos = self.get_recording_layout(layout)\n",
    "            positions[idx, :len(rec_pos)] = rec_pos.to(meg.device)\n",
    "        return positions\n",
    "\n",
    "    def is_invalid(self, positions):\n",
    "        return (positions == self.INVALID).all(dim=-1)\n",
    "\n",
    "class FourierEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier positional embedding.\n",
    "    Unlike trad. embedding this is not using exponential periods\n",
    "    for cosines and sinuses, but typical `2 pi k` which can represent\n",
    "    any function over [0, 1]. As this function would be necessarily periodic,\n",
    "    we take a bit of margin and do over [-0.2, 1.2].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension: int = 256, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        n_freqs = (dimension // 2) ** 0.5\n",
    "        assert int(n_freqs ** 2 * 2) == dimension\n",
    "        self.dimension = dimension\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, positions):\n",
    "        *O, D = positions.shape\n",
    "        assert D == 2\n",
    "        *O, D = positions.shape\n",
    "        n_freqs = (self.dimension // 2) ** 0.5\n",
    "        freqs_y = torch.arange(n_freqs).to(positions)\n",
    "        freqs_x = freqs_y[:, None]\n",
    "        width = 1 + 2 * self.margin\n",
    "        positions = positions + self.margin\n",
    "        p_x = 2 * math.pi * freqs_x / width\n",
    "        p_y = 2 * math.pi * freqs_y / width\n",
    "        positions = positions[..., None, None, :]\n",
    "        loc = (positions[..., 0] * p_x + positions[..., 1] * p_y).view(*O, -1)\n",
    "        emb = torch.cat([\n",
    "            torch.cos(loc),\n",
    "            torch.sin(loc),\n",
    "        ], dim=-1)\n",
    "        return emb\n",
    "\n",
    "class ChannelMerger(nn.Module):\n",
    "    def __init__(self, chout: int, pos_dim: int = 256,\n",
    "                 dropout: float = 0, usage_penalty: float = 0.,\n",
    "                 n_subjects: int = 200, per_subject: bool = False):\n",
    "        super().__init__()\n",
    "        assert pos_dim % 4 == 0\n",
    "        self.position_getter = PositionGetter()\n",
    "        self.per_subject = per_subject\n",
    "        if self.per_subject:\n",
    "            self.heads = nn.Parameter(torch.randn(n_subjects, chout, pos_dim, requires_grad=True))\n",
    "        else:\n",
    "            self.heads = nn.Parameter(torch.randn(chout, pos_dim, requires_grad=True))\n",
    "        self.heads.data /= pos_dim ** 0.5\n",
    "        self.dropout = dropout\n",
    "        self.embedding = FourierEmb(pos_dim)\n",
    "        self.usage_penalty = usage_penalty\n",
    "        self._penalty = torch.tensor(0.)\n",
    "\n",
    "    @property\n",
    "    def training_penalty(self):\n",
    "        return self._penalty.to(next(self.parameters()).device)\n",
    "\n",
    "    def forward(self, meg, batch, layout):\n",
    "        B, C, T = meg.shape\n",
    "        meg = meg.clone()\n",
    "        positions = self.position_getter.get_positions(meg, batch, layout)\n",
    "        embedding = self.embedding(positions)\n",
    "        score_offset = torch.zeros(B, C, device=meg.device)\n",
    "        # score_offset[self.position_getter.is_invalid(positions)] = float('-inf')\n",
    "\n",
    "        if self.training and self.dropout:\n",
    "            center_to_ban = torch.rand(2, device=meg.device)\n",
    "            radius_to_ban = self.dropout\n",
    "            banned = (positions - center_to_ban).norm(dim=-1) <= radius_to_ban\n",
    "            score_offset[banned] = float('-inf')\n",
    "        if self.per_subject:\n",
    "            _, cout, pos_dim = self.heads.shape\n",
    "            subject = batch['subject_index']-1 # -1?\n",
    "            heads = self.heads.gather(0, subject.view(-1, 1, 1).expand(-1, cout, pos_dim))\n",
    "        else:\n",
    "            heads = self.heads[None].expand(B, -1, -1)\n",
    "\n",
    "        scores = torch.einsum(\"bcd,bod->boc\", embedding, heads)\n",
    "        scores += score_offset[:, None]\n",
    "        weights = torch.softmax(scores, dim=2)\n",
    "        out = torch.einsum(\"bct,boc->bot\", meg, weights)\n",
    "        if self.training and self.usage_penalty > 0.:\n",
    "            usage = weights.mean(dim=(0, 1)).sum()\n",
    "            self._penalty = self.usage_penalty * usage\n",
    "        return out\n",
    "\n",
    "class SubjectLayers(nn.Module):\n",
    "    \"\"\"Per subject linear layer.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_subjects: int, init_id: bool = False):\n",
    "        super().__init__()\n",
    "        self.C = in_channels\n",
    "        self.D = out_channels\n",
    "        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels), requires_grad=True)\n",
    "        #self.weights = nn.Embedding(n_subjects, in_channels*out_channels)\n",
    "        if init_id:\n",
    "            assert in_channels == out_channels\n",
    "            self.weights.data[:] = torch.eye(in_channels)[None]\n",
    "        # self.weights.data *= 1 / in_channels ** 0.5\n",
    "    def forward(self, x, subjects):\n",
    "        B = x.shape[0]\n",
    "        weights = self.weights.gather(0, subjects.long().view(-1, 1, 1).expand(-1, self.C, self.D))\n",
    "        #weights = self.weights(subjects).view(B, self.C, self.D)\n",
    "        return torch.einsum(\"bct,bcd->bdt\", x, weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        S, C, D = self.weights.shape\n",
    "        return f\"SubjectLayers({C}, {D}, {S})\"\n",
    "\n",
    "class ConvPreNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # Channels\n",
    "                 in_channels: tp.Dict[str, int] = {\"meg\": 208},\n",
    "                 out_channels: int = 80,\n",
    "                 hidden: tp.Dict[str, int] = {\"meg\": 320},\n",
    "                 # Conv layer\n",
    "                 gelu: bool = True,\n",
    "                 relu_leakiness: float = 0.0,\n",
    "                 # Subject specific settings\n",
    "                 n_subjects: int = 27,\n",
    "                 subject_layers: bool = True,\n",
    "                 subject_layers_dim: str = \"hidden\",  # or hidden\n",
    "                 subject_layers_id: bool = False,\n",
    "                 # Attention multi-dataset support\n",
    "                 merger: bool = True,\n",
    "                 merger_pos_dim: int = 2048,\n",
    "                 merger_channels: int = 270,\n",
    "                 merger_dropout: float = 0.2,\n",
    "                 merger_penalty: float = 0.,\n",
    "                 merger_per_subject: bool = False,\n",
    "                 dropout: float = 0.,\n",
    "                 initial_linear: int = 270,\n",
    "                 initial_depth: int = 1,\n",
    "                 initial_nonlin: bool = False,\n",
    "                 run_name = 'default',\n",
    "                 useful_length = 300,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.run_name= run_name\n",
    "        if set(in_channels.keys()) != set(hidden.keys()):\n",
    "            raise ValueError(\"Channels and hidden keys must match \"\n",
    "                             f\"({set(in_channels.keys())} and {set(hidden.keys())})\")\n",
    "        self.out_channels = out_channels\n",
    "        if gelu:\n",
    "            activation = nn.GELU\n",
    "        elif relu_leakiness:\n",
    "            activation = partial(nn.LeakyReLU, relu_leakiness)\n",
    "        else:\n",
    "            activation = nn.ReLU\n",
    "\n",
    "        \n",
    "        self.layout=loaded_layout # only gwilliams\n",
    "        self.merger = None\n",
    "        if merger:\n",
    "            self.merger = ChannelMerger(\n",
    "                merger_channels, pos_dim=merger_pos_dim, dropout=merger_dropout,\n",
    "                usage_penalty=merger_penalty, n_subjects=n_subjects, per_subject=merger_per_subject)\n",
    "            in_channels[\"meg\"] = merger_channels\n",
    "\n",
    "        self.initial_linear = None\n",
    "        if initial_linear:\n",
    "            init = [nn.Conv1d(in_channels[\"meg\"], initial_linear, 1)]\n",
    "            for _ in range(initial_depth - 1):\n",
    "                init += [activation(), nn.Conv1d(initial_linear, initial_linear, 1)]\n",
    "            if initial_nonlin:\n",
    "                init += [activation()]\n",
    "            self.initial_linear = nn.Sequential(*init)\n",
    "            in_channels[\"meg\"] = initial_linear\n",
    "\n",
    "        self.subject_layers = None\n",
    "        if subject_layers:\n",
    "            assert \"meg\" in in_channels\n",
    "            meg_dim = in_channels[\"meg\"]\n",
    "            dim = {\"hidden\": hidden[\"meg\"], \"input\": meg_dim}[subject_layers_dim]\n",
    "            self.subject_layers = SubjectLayers(meg_dim, dim, n_subjects, subject_layers_id)\n",
    "            in_channels[\"meg\"] = dim\n",
    "        sizes = {'meg': [320, 320, 320, 320, 320, 320]}\n",
    "        self.conv_encoder = nn.ModuleDict({name: ConvSequence(channels)\n",
    "                                           for name, channels in sizes.items()})\n",
    "\n",
    "\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "                useful_length, 320, padding_idx=0\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, batch):\n",
    "        # subj idx,\n",
    "        subjects = batch['subject_index']-1\n",
    "        length = inputs[\"meg\"].shape[2]  # length of any of the inputs\n",
    "\n",
    "        if self.merger is not None:\n",
    "            inputs[\"meg\"] = self.merger(inputs[\"meg\"], batch, self.layout)\n",
    "        \n",
    "        if self.initial_linear is not None:\n",
    "            inputs[\"meg\"] = self.initial_linear(inputs[\"meg\"])\n",
    "\n",
    "        if self.subject_layers is not None:\n",
    "            inputs[\"meg\"] = self.subject_layers(inputs[\"meg\"], subjects)\n",
    "            \n",
    "        inputs[\"meg\"] = self.conv_encoder['meg'](inputs[\"meg\"])\n",
    "        import pdb;pdb.set_trace()\n",
    "        \n",
    "        positions = self.embed_positions(torch.zeros((inputs[\"meg\"].shape[0], inputs[\"meg\"].shape[2]), dtype=torch.bool))\n",
    "        x = inputs[\"meg\"] + positions.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(FairseqEncoder):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n",
    "    is a :class:`TransformerEncoderLayer`.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): parsed command-line arguments\n",
    "        dictionary (~fairseq.data.Dictionary): encoding dictionary\n",
    "        embed_tokens (torch.nn.Embedding): input embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder_layers=4,\n",
    "                 dropout=0.1,\n",
    "                 encoder_layerdrop=0.1,\n",
    "                 freeze_encoder_updates=130000,\n",
    "                 no_freeze_encoder_layer=None,\n",
    "                 use_sent_enc_layer=True,\n",
    "                 unb_enc_layer=-1,\n",
    "                 layer_norm_first=False,\n",
    "                 encoder_embed_dim=320,\n",
    "                 layer_norm_eps=1e-8,\n",
    "                 encoder_attention_heads=8,\n",
    "                 encoder_max_relative_position=1200,\n",
    "                 encoder_ffn_embed_dim=320*4,\n",
    "                 attention_dropout=0.1,\n",
    "                 activation_dropout=0.1,\n",
    "                 activation_fn=\"gelu\",\n",
    "                 relative_position_embedding=True\n",
    "                ):\n",
    "        super().__init__(None)\n",
    "\n",
    "        self.register_buffer(\"version\", torch.Tensor([3]))\n",
    "        self.dropout_module = FairseqDropout(\n",
    "            dropout, module_name=self.__class__.__name__\n",
    "        )\n",
    "        self.encoder_layerdrop = encoder_layerdrop\n",
    "        self.freeze_encoder_updates = freeze_encoder_updates\n",
    "        self.no_freeze_encoder_layer = no_freeze_encoder_layer\n",
    "        self.num_updates = 0\n",
    "        export = False\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend(\n",
    "            [self.build_encoder_layer(use_sent_enc_layer, encoder_embed_dim, encoder_ffn_embed_dim, encoder_attention_heads, \n",
    "                            dropout, attention_dropout, activation_dropout, activation_fn, layer_norm_first, relative_position_embedding) \n",
    "             for i in range(encoder_layers)]\n",
    "        )\n",
    "        self.num_layers = len(self.layers)\n",
    "\n",
    "        self.use_sent_enc_layer = use_sent_enc_layer\n",
    "        self.unb_enc_layer = unb_enc_layer\n",
    "\n",
    "        self.layer_norm_first = layer_norm_first\n",
    "        self.layer_norm = LayerNorm(encoder_embed_dim, eps=layer_norm_eps, export=export)\n",
    "        \n",
    "        self.proj = None\n",
    "        self.relative_position_embedding = relative_position_embedding\n",
    "        if relative_position_embedding:\n",
    "            self.pos_emb = RelativePositionalEncoding(encoder_embed_dim//encoder_attention_heads, encoder_max_relative_position)\n",
    "\n",
    "\n",
    "    def build_encoder_layer(self, \n",
    "                            use_sent_enc_layer, \n",
    "                            encoder_embed_dim, \n",
    "                            encoder_ffn_embed_dim, \n",
    "                            encoder_attention_heads, \n",
    "                            dropout, attention_dropout, \n",
    "                            activation_dropout, \n",
    "                            activation_fn, \n",
    "                            layer_norm_first, \n",
    "                            relative_position_embedding\n",
    "                           ):\n",
    "        if use_sent_enc_layer:\n",
    "            layer = TransformerSentenceEncoderLayer(\n",
    "                embedding_dim=encoder_embed_dim,\n",
    "                ffn_embedding_dim=encoder_ffn_embed_dim,\n",
    "                num_attention_heads=encoder_attention_heads,\n",
    "                dropout=dropout,\n",
    "                attention_dropout=attention_dropout,\n",
    "                activation_dropout=activation_dropout,\n",
    "                activation_fn=activation_fn,\n",
    "                layer_norm_first=layer_norm_first,\n",
    "                has_relative_attention_bias=relative_position_embedding,\n",
    "            )\n",
    "        else:\n",
    "            layer = TransformerEncoderLayer()\n",
    "        return layer\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_in,\n",
    "        encoder_padding_mask,\n",
    "        return_all_hiddens: bool = False,\n",
    "        tgt_layer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_tokens (LongTensor): tokens in the source language of shape\n",
    "                `(batch, src_len)`\n",
    "            src_lengths (torch.LongTensor): lengths of each source sentence of\n",
    "                shape `(batch)`\n",
    "            return_all_hiddens (bool, optional): also return all of the\n",
    "                intermediate hidden states (default: False).\n",
    "            token_embeddings (torch.Tensor, optional): precomputed embeddings\n",
    "                default `None` will recompute embeddings\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                - **encoder_out** (Tensor): the last encoder layer's output of\n",
    "                  shape `(src_len, batch, embed_dim)`\n",
    "                - **encoder_padding_mask** (ByteTensor): the positions of\n",
    "                  padding elements of shape `(batch, src_len)`\n",
    "                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n",
    "                  of shape `(batch, src_len, embed_dim)`\n",
    "                - **encoder_states** (List[Tensor]): all intermediate\n",
    "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
    "                  Only populated if *return_all_hiddens* is True.\n",
    "        \"\"\"\n",
    "        if self.no_freeze_encoder_layer is None:\n",
    "            ft = self.num_updates <= self.freeze_encoder_updates\n",
    "        else:\n",
    "            ft = True\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
    "            encoder_out = self.forward_scriptable(\n",
    "                encoder_in, encoder_padding_mask, return_all_hiddens, tgt_layer=tgt_layer,\n",
    "            )\n",
    "\n",
    "        # CTC and bert\n",
    "        if self.proj:\n",
    "            x_for_ctc = self.proj(self.dropout_module(encoder_out[\"encoder_out\"][0]))\n",
    "        else:\n",
    "            x_for_ctc = None\n",
    "\n",
    "        encoder_out[\"encoder_out_for_ctc\"] = [x_for_ctc] # T x B x C\n",
    "\n",
    "        return encoder_out\n",
    "\n",
    "    # TorchScript doesn't support super() method so that the scriptable Subclass\n",
    "    # can't access the base class model in Torchscript.\n",
    "    # Current workaround is to add a helper function with different name and\n",
    "    # call the helper function from scriptable Subclass.\n",
    "    def forward_scriptable(\n",
    "        self,\n",
    "        encoder_in,\n",
    "        encoder_padding_mask,\n",
    "        return_all_hiddens: bool = False,\n",
    "        tgt_layer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_tokens (LongTensor): tokens in the source language of shape\n",
    "                `(batch, src_len)`\n",
    "            src_lengths (torch.LongTensor): lengths of each source sentence of\n",
    "                shape `(batch)`\n",
    "            return_all_hiddens (bool, optional): also return all of the\n",
    "                intermediate hidden states (default: False).\n",
    "            token_embeddings (torch.Tensor, optional): precomputed embeddings\n",
    "                default `None` will recompute embeddings\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                - **encoder_out** (Tensor): the last encoder layer's output of\n",
    "                  shape `(src_len, batch, embed_dim)`\n",
    "                - **encoder_padding_mask** (ByteTensor): the positions of\n",
    "                  padding elements of shape `(batch, src_len)`\n",
    "                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n",
    "                  of shape `(batch, src_len, embed_dim)`\n",
    "                - **encoder_states** (List[Tensor]): all intermediate\n",
    "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
    "                  Only populated if *return_all_hiddens* is True.\n",
    "        \"\"\"\n",
    "        if self.no_freeze_encoder_layer is not None:\n",
    "            ft = self.freeze_encoder_updates <= self.num_updates\n",
    "        else:\n",
    "            ft = True\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
    "            # compute padding mask\n",
    "            if not self.use_sent_enc_layer:\n",
    "                has_pads = encoder_in.device.type == \"xla\" or encoder_padding_mask.any()\n",
    "\n",
    "            if not self.layer_norm_first:\n",
    "                encoder_in = self.layer_norm(encoder_in)\n",
    "\n",
    "            encoder_in = self.dropout_module(encoder_in)\n",
    "\n",
    "            # B x T x C -> T x B x C\n",
    "            x = encoder_in.permute(1, 0, 2)\n",
    "\n",
    "            encoder_states = []\n",
    "\n",
    "            if return_all_hiddens:\n",
    "                encoder_states.append(x)\n",
    "\n",
    "            ## relative position embedding\n",
    "            if self.relative_position_embedding:\n",
    "                x_len = x.shape[0]\n",
    "                pos_seq = torch.arange(0, x_len).long().to(x.device)\n",
    "                pos_seq = pos_seq[:, None] - pos_seq[None, :]\n",
    "                pos_k, pos_v = self.pos_emb(pos_seq)\n",
    "            else:\n",
    "                pos_k = None\n",
    "\n",
    "        # encoder layers\n",
    "        r = None\n",
    "        d = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            dropout_probability = np.random.random()\n",
    "\n",
    "            with torch.no_grad() if (not ft) and i not in self.no_freeze_encoder_layer else contextlib.ExitStack():\n",
    "                if not self.training or (dropout_probability > self.encoder_layerdrop) or i == self.unb_enc_layer:\n",
    "                    if self.use_sent_enc_layer:\n",
    "                        x, _ = layer(x, self_attn_padding_mask=encoder_padding_mask, self_attn_mask=None, need_weights=False, pos_bias=pos_k)\n",
    "                    else:\n",
    "                        x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, attn_mask=None)\n",
    "                if i == self.unb_enc_layer:\n",
    "                    d = x\n",
    "\n",
    "                if i == tgt_layer:\n",
    "                    r = x\n",
    "                    break\n",
    "\n",
    "                if return_all_hiddens:\n",
    "                    assert encoder_states is not None\n",
    "                    encoder_states.append(x)\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
    "            # Finally T x B x C\n",
    "            if self.layer_norm_first:\n",
    "                x = self.layer_norm(x.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "            if r is not None:\n",
    "                x = r\n",
    "\n",
    "        # The Pytorch Mobile lite interpreter does not supports returning NamedTuple in\n",
    "        # `forward` so we use a dictionary instead.\n",
    "        # TorchScript does not support mixed values so the values are all lists.\n",
    "        # The empty list is equivalent to None.\n",
    "        return {\n",
    "            \"encoder_out\": [x],  # T x B x C\n",
    "            \"encoder_padding_mask\": [encoder_padding_mask],  # B x T\n",
    "            \"encoder_states\": encoder_states,  # List[T x B x C]\n",
    "            \"src_tokens\": [],\n",
    "            \"decoder_input\": [d],\n",
    "        }\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n",
    "        for i in range(self.num_layers):\n",
    "            # update layer norms\n",
    "            if not isinstance(self.layers[i], TransformerSentenceEncoderLayer):\n",
    "                self.layers[i].upgrade_state_dict_named(\n",
    "                    state_dict, \"{}.layers.{}\".format(name, i)\n",
    "                )\n",
    "\n",
    "        version_key = \"{}.version\".format(name)\n",
    "        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\n",
    "            # earlier checkpoints did not normalize after the stack of layers\n",
    "            self.layer_norm = None\n",
    "            self.normalize = False\n",
    "            state_dict[version_key] = torch.Tensor([1])\n",
    "        return state_dict\n",
    "\n",
    "    def set_num_updates(self, num_updates):\n",
    "        \"\"\"Set the number of parameters updates.\"\"\"\n",
    "        super().set_num_updates(num_updates)\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "\n",
    "class TransformerSentenceEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: float = 768,\n",
    "        ffn_embedding_dim: float = 3072,\n",
    "        num_attention_heads: float = 8,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        activation_dropout: float = 0.1,\n",
    "        activation_fn: str = \"relu\",\n",
    "        layer_norm_first: bool = False,\n",
    "        has_relative_attention_bias: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        # Initialize parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "\n",
    "        # Initialize blocks\n",
    "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            self.embedding_dim,\n",
    "            num_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            self_attention=True,\n",
    "            has_relative_attention_bias=has_relative_attention_bias,\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm_first = layer_norm_first\n",
    "\n",
    "        # layer norm associated with the self attention layer\n",
    "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
    "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
    "\n",
    "        # layer norm associated with the position wise feed-forward NN\n",
    "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
    "\n",
    "        if has_relative_attention_bias:\n",
    "            self.norm_k = LayerNorm(self.embedding_dim//num_attention_heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        self_attn_mask: torch.Tensor = None,\n",
    "        self_attn_padding_mask: torch.Tensor = None,\n",
    "        need_weights: bool = False,\n",
    "        att_args=None,\n",
    "        pos_bias=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LayerNorm is applied either before or after the self-attention/ffn\n",
    "        modules similar to the original Transformer imlementation.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        if self.layer_norm_first:\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "            if pos_bias is not None:\n",
    "                pos_bias = self.norm_k(pos_bias)\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "                attn_mask=self_attn_mask,\n",
    "                position_bias=pos_bias,\n",
    "            )\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            residual = x\n",
    "            x = self.final_layer_norm(x)\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "        else:\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "                position_bias=pos_bias,\n",
    "            )\n",
    "\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "\n",
    "            residual = x\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "            x = self.final_layer_norm(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "class RelativePositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, maxlen=1000, embed_v=False):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.maxlen = maxlen\n",
    "        self.pe_k = torch.nn.Embedding(2*maxlen, d_model) \n",
    "        if embed_v:\n",
    "            self.pe_v = torch.nn.Embedding(2*maxlen, d_model)\n",
    "        self.embed_v = embed_v\n",
    "\n",
    "    def forward(self, pos_seq):\n",
    "        pos_seq[pos_seq < -self.maxlen] = -self.maxlen\n",
    "        pos_seq[pos_seq >= self.maxlen] = self.maxlen - 1\n",
    "        pos_seq = pos_seq + self.maxlen\n",
    "        if self.embed_v:\n",
    "            return self.pe_k(pos_seq), self.pe_v(pos_seq)\n",
    "        else:\n",
    "            return self.pe_k(pos_seq), None\n",
    "\n",
    "def Linear(in_features, out_features, bias=True):\n",
    "    m = nn.Linear(in_features, out_features, bias)\n",
    "    nn.init.xavier_uniform_(m.weight)\n",
    "    if bias:\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "    return m\n",
    "\n",
    "class Postnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        idim,\n",
    "        odim,\n",
    "        n_layers=3,\n",
    "        n_chans=320,\n",
    "        n_filts=5,\n",
    "        use_batch_norm=True,\n",
    "    ):\n",
    "        \"\"\"Initialize postnet module.\n",
    "\n",
    "        Args:\n",
    "            idim (int): Dimension of the inputs.\n",
    "            odim (int): Dimension of the outputs.\n",
    "            n_layers (int, optional): The number of layers.\n",
    "            n_filts (int, optional): The number of filter size.\n",
    "            n_units (int, optional): The number of filter channels.\n",
    "            use_batch_norm (bool, optional): Whether to use batch normalization..\n",
    "            dropout_rate (float, optional): Dropout rate..\n",
    "\n",
    "        \"\"\"\n",
    "        super(Postnet, self).__init__()\n",
    "        self.postnet = torch.nn.ModuleList()\n",
    "        for layer in range(n_layers - 1):\n",
    "            ichans = idim if layer == 0 else n_chans\n",
    "            ochans = odim if layer == n_layers - 1 else n_chans\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                self.postnet += [\n",
    "                    torch.nn.Sequential(\n",
    "                        torch.nn.Conv1d(\n",
    "                            ichans,\n",
    "                            ochans,\n",
    "                            n_filts,\n",
    "                            stride=1,\n",
    "                            padding=(n_filts - 1) // 2,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                        torch.nn.BatchNorm1d(ochans),\n",
    "                        torch.nn.Tanh(),\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                self.postnet += [\n",
    "                    torch.nn.Sequential(\n",
    "                        torch.nn.Conv1d(\n",
    "                            ichans,\n",
    "                            ochans,\n",
    "                            n_filts,\n",
    "                            stride=1,\n",
    "                            padding=(n_filts - 1) // 2,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                        torch.nn.GELU(),\n",
    "                    )\n",
    "                ]\n",
    "        ichans = n_chans if n_layers != 1 else odim\n",
    "        if use_batch_norm:\n",
    "            self.postnet += [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(\n",
    "                        ichans,\n",
    "                        odim,\n",
    "                        n_filts,\n",
    "                        stride=1,\n",
    "                        padding=(n_filts - 1) // 2,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm1d(odim),\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            self.postnet += [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(\n",
    "                        ichans,\n",
    "                        odim,\n",
    "                        n_filts,\n",
    "                        stride=1,\n",
    "                        padding=(n_filts - 1) // 2,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "\n",
    "        Args:\n",
    "            xs (Tensor): Batch of the sequences of padded input tensors (B, idim, Tmax).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Batch of padded output tensor. (B, odim, Tmax).\n",
    "\n",
    "        \"\"\"      \n",
    "        for i in range(len(self.postnet)):\n",
    "            xs = self.postnet[i](xs)\n",
    "        return xs\n",
    "\n",
    "class ConformerEncoder(FairseqEncoder):\n",
    "    \"\"\"Conformer Encoder for speech translation based on https://arxiv.org/abs/2005.08100\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder_embed_dim=320,\n",
    "                 no_scale_embedding=True,\n",
    "                 dropout=0.1,\n",
    "                 encoder_ffn_embed_dim=320*4,\n",
    "                 encoder_attention_heads=8,\n",
    "                 depthwise_conv_kernel_size=31,\n",
    "                 attn_type=None,\n",
    "                 fp16=False,\n",
    "                 encoder_layers=4,\n",
    "                ):\n",
    "        super().__init__(None)\n",
    "        self.embed_scale = math.sqrt(encoder_embed_dim)\n",
    "        if no_scale_embedding:\n",
    "            self.embed_scale = 1.0\n",
    "        self.pos_enc_type = \"abs\"\n",
    "        self.linear = torch.nn.Linear(encoder_embed_dim, encoder_embed_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conformer_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                ConformerEncoderLayer(\n",
    "                    embed_dim=encoder_embed_dim,\n",
    "                    ffn_embed_dim=encoder_ffn_embed_dim,\n",
    "                    attention_heads=encoder_attention_heads,\n",
    "                    dropout=dropout,\n",
    "                    depthwise_conv_kernel_size=depthwise_conv_kernel_size,\n",
    "                    attn_type=attn_type,\n",
    "                    pos_enc_type=self.pos_enc_type,\n",
    "                    use_fp16=fp16,\n",
    "                )\n",
    "                for _ in range(encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask, return_all_hiddens=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_tokens: Input source tokens Tensor of shape B X T X C\n",
    "            src_lengths: Lengths Tensor corresponding to input source tokens\n",
    "            return_all_hiddens: If true will append the self attention states to the encoder states\n",
    "        Returns:\n",
    "            encoder_out: Tensor of shape B X T X C\n",
    "            encoder_padding_mask: Optional Tensor with mask\n",
    "            encoder_embedding: Optional Tensor. Always empty here\n",
    "            encoder_states: List of Optional Tensors wih self attention states\n",
    "            src_tokens: Optional Tensor. Always empty here\n",
    "            src_lengths: Optional Tensor. Always empty here\n",
    "        \"\"\"\n",
    "        import pdb;pdb.set_trace()\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.embed_scale * x\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        encoder_states = []\n",
    "        positions = None\n",
    "        # x is T X B X C\n",
    "        for layer in self.conformer_layers:\n",
    "            x, _ = layer(x, encoder_padding_mask, positions)\n",
    "            if return_all_hiddens:\n",
    "                encoder_states.append(x)\n",
    "\n",
    "        return {\n",
    "            \"encoder_out\": [x],  # T x B x C\n",
    "            \"encoder_padding_mask\": [encoder_padding_mask]\n",
    "            if encoder_padding_mask.any()\n",
    "            else [],  # B x T\n",
    "            \"encoder_embedding\": [],  # B x T x C\n",
    "            \"encoder_states\": encoder_states,  # List[T x B x C]\n",
    "            \"src_tokens\": [],\n",
    "            \"src_lengths\": [],\n",
    "        }\n",
    "\n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        \"\"\"Required method for a FairseqEncoder. Calls the method from the parent class\"\"\"\n",
    "        return S2TTransformerEncoder.reorder_encoder_out(self, encoder_out, new_order)\n",
    "\n",
    "class BrainTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: dict,\n",
    "        run_name: str = 'test',\n",
    "        depth: int = 4,\n",
    "        useful_length: int = 300,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_layer = ConvPreNet(in_channels=in_channels, run_name=run_name, useful_length=useful_length)\n",
    "        self.transformer = ConformerEncoder(encoder_layers=depth)\n",
    "        self.post_layer = Postnet(idim=320,odim=80)\n",
    "        # self.post_layer = nn.Sequential(\n",
    "        #         nn.Conv1d(320, 2 * 320, 1),\n",
    "        #         nn.GELU(),\n",
    "        #         nn.ConvTranspose1d(320 * 2, 80, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: dict, \n",
    "        batch: dict,\n",
    "    ):\n",
    "        import pdb;pdb.set_trace()\n",
    "        x = self.pre_layer(inputs=inputs, batch=batch).permute(0, 2, 1)\n",
    "        x = self.transformer(x, (torch.arange(inputs[\"meg\"].shape[2]).expand(inputs[\"meg\"].shape[0], -1) >= 1000))\n",
    "        out = self.post_layer(x[\"encoder_out\"][0].permute(1, 2, 0))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a658b60-bbda-446c-85b4-50bd6060a0f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16479730\n"
     ]
    }
   ],
   "source": [
    "meg = torch.randn(32, 208, 1200)\n",
    "sub_id = torch.randint(1, 27, size=(32, 1, 1))\n",
    "brain_module = BrainTransformer(in_channels={\"meg\": 208}, run_name='test', depth=4, useful_length=1200)\n",
    "total_p = sum(p.numel() for p in brain_module.parameters())\n",
    "print(total_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba362a-73c1-4898-8d19-43b011e770f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_250335/2024497697.py\u001b[0m(976)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    974 \u001b[0;31m    ):\n",
      "\u001b[0m\u001b[0;32m    975 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 976 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    977 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    978 \u001b[0;31m        \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_250335/2024497697.py\u001b[0m(355)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    353 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    354 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 355 \u001b[0;31m        \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    356 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    357 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_250335/2024497697.py\u001b[0m(924)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    922 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m    923 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 924 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    925 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    926 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/petrelfs/zhangchi1/miniconda3/envs/mad/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m(5309)\u001b[0;36mmulti_head_attention_forward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   5307 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5308 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 5309 \u001b[0;31m    key_padding_mask = _canonical_mask(\n",
      "\u001b[0m\u001b[0;32m   5310 \u001b[0;31m        \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5311 \u001b[0;31m        \u001b[0mmask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"key_padding_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  key_padding_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/petrelfs/zhangchi1/miniconda3/envs/mad/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m(5310)\u001b[0;36mmulti_head_attention_forward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   5308 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5309 \u001b[0;31m    key_padding_mask = _canonical_mask(\n",
      "\u001b[0m\u001b[0;32m-> 5310 \u001b[0;31m        \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5311 \u001b[0;31m        \u001b[0mmask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"key_padding_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5312 \u001b[0;31m        \u001b[0mother_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_none_or_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/petrelfs/zhangchi1/miniconda3/envs/mad/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m(5311)\u001b[0;36mmulti_head_attention_forward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   5309 \u001b[0;31m    key_padding_mask = _canonical_mask(\n",
      "\u001b[0m\u001b[0;32m   5310 \u001b[0;31m        \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 5311 \u001b[0;31m        \u001b[0mmask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"key_padding_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5312 \u001b[0;31m        \u001b[0mother_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_none_or_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5313 \u001b[0;31m        \u001b[0mother_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/petrelfs/zhangchi1/miniconda3/envs/mad/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m(5312)\u001b[0;36mmulti_head_attention_forward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   5310 \u001b[0;31m        \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5311 \u001b[0;31m        \u001b[0mmask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"key_padding_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 5312 \u001b[0;31m        \u001b[0mother_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_none_or_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5313 \u001b[0;31m        \u001b[0mother_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   5314 \u001b[0;31m        \u001b[0mtarget_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  query.dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "out = brain_module({\"meg\": meg}, {\"subject_index\": sub_id})\n",
    "import pdb;pdb.set_trace()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced17cd-2083-4b6b-86b5-f6aa43216cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
